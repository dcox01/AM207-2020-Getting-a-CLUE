{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COMPAS_Data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"fs8tFimErJFl"},"source":["from __future__ import division\r\n","import urllib.request\r\n","import os,sys\r\n","import numpy as np\r\n","import pandas as pd\r\n","from collections import defaultdict\r\n","from sklearn import feature_extraction\r\n","from sklearn import preprocessing\r\n","from random import seed, shuffle\r\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFDuBMiLraYc"},"source":["data = pd.read_csv('https://raw.githubusercontent.com/rclarkbar/hw/main/compas-scores-two-years.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EnveNtfsmpBp"},"source":["SEED = 1234\r\n","seed(SEED)\r\n","np.random.seed(SEED)\r\n","\r\n","\r\n","def add_intercept(x):\r\n","    \"\"\" Add intercept to the data before linear classification \"\"\"\r\n","    m,n = x.shape\r\n","    intercept = np.ones(m).reshape(m, 1) # the constant b\r\n","    return np.concatenate((intercept, x), axis = 1)\r\n","\r\n","def load_compas_data(data):\r\n","\r\n","\tFEATURES_CLASSIFICATION = [\"age\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\",\"days_served\"] #features to be used for classification\r\n","\tCONT_VARIABLES = [\"priors_count\",\"age\",\"days_served\"] # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\r\n","\tCLASS_FEATURE = \"two_year_recid\" # the decision variable\r\n","\tSENSITIVE_ATTRS = [\"race\"]\r\n","\r\n","\t# load the data and get some stats\r\n","\tdf = data.copy()\r\n","\tdf = df.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\r\n","\t\r\n","\t# convert to np array\r\n","\tdata = df.to_dict('list')\r\n","\tfor k in data.keys():\r\n","\t\tdata[k] = np.array(data[k])\r\n","\r\n","\r\n","\t\"\"\" Filtering the data \"\"\"\r\n","\r\n","\t# These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\r\n","\t# If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense. \r\n","\tidx = np.logical_and(data[\"days_b_screening_arrest\"]<=30, data[\"days_b_screening_arrest\"]>=-30)\r\n","\r\n","\r\n","\t# We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\r\n","\tidx = np.logical_and(idx, data[\"is_recid\"] != -1)\r\n","\r\n","\t# In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\r\n","\tidx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\") # F: felony, M: misconduct\r\n","\r\n","\t# We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\r\n","\tidx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\r\n","\r\n","\t# we will only consider blacks and whites for this analysis\r\n","\tidx = np.logical_and(idx, np.logical_or(data[\"race\"] == \"African-American\", data[\"race\"] == \"Caucasian\"))\r\n","\r\n","\t# select the examples that satisfy this criteria\r\n","\tfor k in data.keys():\r\n","\t\tdata[k] = data[k][idx]\r\n","\r\n","\r\n","\r\n","\t\"\"\" Feature normalization and one hot encoding \"\"\"\r\n","\r\n","\ty = data[CLASS_FEATURE]\r\n","\t\r\n","\tprint(\"\\nNumber of people recidivating within two years\")\r\n","\tprint(pd.Series(y).value_counts())\r\n","\tprint(\"\\n\")\r\n","\r\n","\r\n","\tX = np.array([]).reshape(len(y), 0) # empty array with num rows same as num examples, will hstack the features to it\r\n","\tx_control = defaultdict(list)\r\n","\r\n","\tfeature_names = []\r\n","\tfor attr in FEATURES_CLASSIFICATION:\r\n","\t\tvals = data[attr]\r\n","\t\tif attr in CONT_VARIABLES:\r\n","\t\t\tvals = [float(v) for v in vals]\r\n","\t\t\t#vals = preprocessing.scale(vals) # 0 mean and 1 variance  \r\n","\t\t\tvals = np.reshape(vals, (len(y), -1)) # convert from 1-d arr to a 2-d arr with one col\r\n","\r\n","\t\telse: # for binary categorical variables, the label binarizer uses just one var instead of two\r\n","\t\t\tlb = preprocessing.LabelBinarizer()\r\n","\t\t\tlb.fit(vals)\r\n","\t\t\tvals = lb.transform(vals)\r\n","\r\n","\t\t# add to sensitive features dict\r\n","\t\tif attr in SENSITIVE_ATTRS:\r\n","\t\t\tx_control[attr] = vals\r\n","\r\n","\r\n","\t\t# add to learnable features\r\n","\t\tX = np.hstack((X, vals))\r\n","\r\n","\t\tif attr in CONT_VARIABLES: # continuous feature, just append the name\r\n","\t\t\tfeature_names.append(attr)\r\n","\t\telse: # categorical features\r\n","\t\t\tif vals.shape[1] == 1: # binary features that passed through lib binarizer\r\n","\t\t\t\tfeature_names.append(attr)\r\n","\t\t\telse:\r\n","\t\t\t\tfor k in lb.classes_: # non-binary categorical features, need to add the names for each cat\r\n","\t\t\t\t\tfeature_names.append(attr + \"_\" + str(k))\r\n","\r\n","\r\n","\t# convert the sensitive feature to 1-d array\r\n","\tx_control = dict(x_control)\r\n","\tfor k in x_control.keys():\r\n","\t\tassert(x_control[k].shape[1] == 1) # make sure that the sensitive feature is binary after one hot encoding\r\n","\t\tx_control[k] = np.array(x_control[k]).flatten()\r\n","\r\n","\t# sys.exit(1)\r\n","\r\n","\t\"\"\"permute the date randomly\"\"\"\r\n","\tperm = list(range(0,X.shape[0]))\r\n","\tshuffle(perm)\r\n","\tX = X[perm]\r\n","\ty = y[perm]\r\n","\tfor k in x_control.keys():\r\n","\t\tx_control[k] = x_control[k][perm]\r\n","\r\n","\tassert(len(feature_names) == X.shape[1])\r\n","\tprint(\"Features we will be using for classification are:\", feature_names, \"\\n\")\r\n","\r\n","\r\n","\treturn X, y, feature_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"THM5VRCUpImM","executionInfo":{"status":"ok","timestamp":1607889390249,"user_tz":300,"elapsed":1981,"user":{"displayName":"Robert Clark","photoUrl":"","userId":"08159530660868951033"}},"outputId":"3bc0ed08-7c94-4fd6-c536-36f233295d61"},"source":["X, y, feature_names = load_compas_data(data)\r\n","print(X.shape)\r\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Number of people recidivating within two years\n","0    2795\n","1    2483\n","dtype: int64\n","\n","\n","Features we will be using for classification are: ['age', 'race', 'sex', 'priors_count', 'c_charge_degree', 'days_served'] \n","\n","(5278, 6)\n","(5278,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uZGjz7do8O7w"},"source":["df = pd.DataFrame(np.concatenate((X,np.asmatrix(y).T),axis=1))\r\n","df.columns = np.append(feature_names,['two_year_recid'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"id":"dnWjT-5z7_64","executionInfo":{"status":"ok","timestamp":1607889390253,"user_tz":300,"elapsed":1973,"user":{"displayName":"Robert Clark","photoUrl":"","userId":"08159530660868951033"}},"outputId":"fa2a9bd7-b82f-42ee-b65f-696db658a9a1"},"source":["df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>priors_count</th>\n","      <th>c_charge_degree</th>\n","      <th>days_served</th>\n","      <th>two_year_recid</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>28.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>34.91</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>27.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>1.0</td>\n","      <td>6.30</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>28.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>21.0</td>\n","      <td>0.0</td>\n","      <td>1.06</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>11.19</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>19.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>22.27</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5273</th>\n","      <td>30.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>15.0</td>\n","      <td>0.0</td>\n","      <td>-0.20</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5274</th>\n","      <td>27.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.29</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5275</th>\n","      <td>35.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.85</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5276</th>\n","      <td>19.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>14.74</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5277</th>\n","      <td>44.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.33</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5278 rows Ã— 7 columns</p>\n","</div>"],"text/plain":["       age  race  sex  ...  c_charge_degree  days_served  two_year_recid\n","0     28.0   0.0  1.0  ...              0.0        34.91             0.0\n","1     27.0   0.0  1.0  ...              1.0         6.30             1.0\n","2     28.0   0.0  1.0  ...              0.0         1.06             0.0\n","3     29.0   1.0  1.0  ...              1.0        11.19             0.0\n","4     19.0   1.0  0.0  ...              0.0        22.27             1.0\n","...    ...   ...  ...  ...              ...          ...             ...\n","5273  30.0   0.0  0.0  ...              0.0        -0.20             1.0\n","5274  27.0   0.0  1.0  ...              0.0         1.29             1.0\n","5275  35.0   0.0  0.0  ...              1.0         0.85             0.0\n","5276  19.0   0.0  1.0  ...              0.0        14.74             1.0\n","5277  44.0   1.0  1.0  ...              1.0         1.33             0.0\n","\n","[5278 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"2h-ApAND-mhZ"},"source":["train, test = train_test_split(df, test_size=1/7,stratify=df.two_year_recid, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lABzaKwgABeA","executionInfo":{"status":"ok","timestamp":1607889390463,"user_tz":300,"elapsed":2173,"user":{"displayName":"Robert Clark","photoUrl":"","userId":"08159530660868951033"}},"outputId":"42bfbc5c-7efa-4966-b9d5-70091eabddfe"},"source":["print(len(train[train.two_year_recid==1])/len(train))\r\n","print(len(train))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.47038019451812557\n","4524\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AknbMKsAUtC","executionInfo":{"status":"ok","timestamp":1607889390465,"user_tz":300,"elapsed":2169,"user":{"displayName":"Robert Clark","photoUrl":"","userId":"08159530660868951033"}},"outputId":"4d8a40e2-500a-4fcb-b3f9-4c793e268c20"},"source":["print(len(test[test.two_year_recid==1])/len(test))\r\n","print(len(test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.4708222811671088\n","754\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PT9MDgSrCHW6"},"source":["# from google.colab import drive\r\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sAKzd5ZSFM1z"},"source":["loc = '/content/drive/Shared drives/AM207 - CLUE Final Project/Dataset/COMPAS/'\r\n","train.to_csv(loc + 'compas_train.csv', index=False)\r\n","test.to_csv(loc + 'compas_test.csv', index=False)"],"execution_count":null,"outputs":[]}]}